"""
å¢å¼ºç‰ˆFastAPIåº”ç”¨å…¥å£
åŸºäºserver.pyçš„æˆç†Ÿå®ç°ï¼Œæ•´åˆå®Œæ•´çš„APIç«¯ç‚¹å’Œæµå¼å¤„ç†
"""

import asyncio
import json
import logging
import os
import sys
import time
import uuid
from contextlib import asynccontextmanager
from datetime import datetime
from typing import AsyncGenerator, Dict, Any, List, Optional, Union, Literal

import litellm
import uvicorn
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel, Field, field_validator

from .config import settings
from .utils import (
    model_manager, error_handler, retry_manager, streaming_handler,
    HealthChecker, LoggingManager, create_request_id, parse_tool_result_content
)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
LoggingManager.setup_logging()
logger = logging.getLogger(__name__)


# ===== Pydanticæ¨¡å‹å®šä¹‰ =====
class ContentBlockText(BaseModel):
    type: Literal["text"] = "text"
    text: str


class ContentBlockImage(BaseModel):
    type: Literal["image"] = "image"
    source: Dict[str, Any]


class ContentBlockToolUse(BaseModel):
    type: Literal["tool_use"] = "tool_use"
    id: str
    name: str
    input: Dict[str, Any]


class ContentBlockToolResult(BaseModel):
    type: Literal["tool_result"] = "tool_result"
    tool_use_id: str
    content: Union[str, List[Dict[str, Any]], Dict[str, Any]]


class SystemContent(BaseModel):
    type: Literal["text"] = "text"
    text: str


class Message(BaseModel):
    role: Literal["user", "assistant"] = Field(..., description="æ¶ˆæ¯è§’è‰²")
    content: Union[str, List[Union[ContentBlockText, ContentBlockImage, ContentBlockToolUse, ContentBlockToolResult]]] = Field(..., description="æ¶ˆæ¯å†…å®¹")


class Tool(BaseModel):
    name: str = Field(..., description="å·¥å…·åç§°")
    description: Optional[str] = Field(None, description="å·¥å…·æè¿°")
    input_schema: Dict[str, Any] = Field(..., description="è¾“å…¥å‚æ•°æ¶æ„")


class ThinkingConfig(BaseModel):
    enabled: bool = Field(True, description="æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼")


class MessagesRequest(BaseModel):
    model: str = Field(..., description="è¯·æ±‚çš„æ¨¡å‹åç§°")
    max_tokens: int = Field(..., description="æœ€å¤§tokenæ•°")
    messages: List[Message] = Field(..., description="æ¶ˆæ¯åˆ—è¡¨")
    system: Optional[Union[str, List[SystemContent]]] = Field(None, description="ç³»ç»Ÿæç¤º")
    stop_sequences: Optional[List[str]] = Field(None, description="åœæ­¢åºåˆ—")
    stream: Optional[bool] = Field(False, description="æ˜¯å¦å¯ç”¨æµå¼å“åº”")
    temperature: Optional[float] = Field(1.0, ge=0.0, le=2.0, description="æ¸©åº¦å‚æ•°")
    top_p: Optional[float] = Field(None, ge=0.0, le=1.0, description="top_pé‡‡æ ·å‚æ•°")
    top_k: Optional[int] = Field(None, ge=1, description="top_ké‡‡æ ·å‚æ•°")
    metadata: Optional[Dict[str, Any]] = Field(None, description="å…ƒæ•°æ®")
    tools: Optional[List[Tool]] = Field(None, description="å·¥å…·åˆ—è¡¨")
    tool_choice: Optional[Dict[str, Any]] = Field(None, description="å·¥å…·é€‰æ‹©é…ç½®")
    thinking: Optional[ThinkingConfig] = Field(None, description="æ€è€ƒé…ç½®")
    original_model: Optional[str] = Field(None, description="åŸå§‹æ¨¡å‹åç§°")

    @field_validator('model')
    @classmethod
    def validate_model_field(cls, v):
        """éªŒè¯å¹¶æ˜ å°„æ¨¡å‹åç§°"""
        original_model = v
        mapped_model, was_mapped = model_manager.validate_and_map_model(v)
        
        logger.debug(f"æ¨¡å‹éªŒè¯: åŸå§‹='{original_model}', æ˜ å°„å='{mapped_model}', æ˜¯å¦æ˜ å°„={was_mapped}")
        
        return mapped_model


class TokenCountRequest(BaseModel):
    model: str = Field(..., description="æ¨¡å‹åç§°")
    messages: List[Message] = Field(..., description="æ¶ˆæ¯åˆ—è¡¨")
    system: Optional[Union[str, List[SystemContent]]] = Field(None, description="ç³»ç»Ÿæç¤º")
    tools: Optional[List[Tool]] = Field(None, description="å·¥å…·åˆ—è¡¨")
    thinking: Optional[ThinkingConfig] = Field(None, description="æ€è€ƒé…ç½®")
    original_model: Optional[str] = Field(None, description="åŸå§‹æ¨¡å‹åç§°")

    @field_validator('model')
    @classmethod
    def validate_model_token_count(cls, v):
        """éªŒè¯tokenè®¡æ•°çš„æ¨¡å‹åç§°"""
        mapped_model, _ = model_manager.validate_and_map_model(v)
        return mapped_model


class Usage(BaseModel):
    input_tokens: int = Field(0, description="è¾“å…¥tokenæ•°")
    output_tokens: int = Field(0, description="è¾“å‡ºtokenæ•°")
    cache_creation_input_tokens: int = Field(0, description="ç¼“å­˜åˆ›å»ºè¾“å…¥tokenæ•°")
    cache_read_input_tokens: int = Field(0, description="ç¼“å­˜è¯»å–è¾“å…¥tokenæ•°")


class MessagesResponse(BaseModel):
    id: str = Field(..., description="å“åº”ID")
    model: str = Field(..., description="ä½¿ç”¨çš„æ¨¡å‹")
    role: Literal["assistant"] = "assistant"
    content: List[Union[ContentBlockText, ContentBlockToolUse]] = Field(..., description="å“åº”å†…å®¹")
    type: Literal["message"] = "message"
    stop_reason: Optional[Literal["end_turn", "max_tokens", "stop_sequence", "tool_use", "error"]] = Field(None, description="åœæ­¢åŸå› ")
    stop_sequence: Optional[str] = Field(None, description="åœæ­¢åºåˆ—")
    usage: Usage = Field(..., description="ä½¿ç”¨ç»Ÿè®¡")


class TokenCountResponse(BaseModel):
    input_tokens: int = Field(..., description="è¾“å…¥tokenæ•°")


# ===== è¯·æ±‚/å“åº”è½¬æ¢ =====
class RequestConverter:
    """è¯·æ±‚æ ¼å¼è½¬æ¢å™¨ï¼ŒåŸºäºserver.pyå®ç°"""
    
    @staticmethod
    def convert_anthropic_to_litellm(anthropic_request: MessagesRequest) -> Dict[str, Any]:
        """å°†Anthropic APIè¯·æ±‚è½¬æ¢ä¸ºLiteLLMæ ¼å¼"""
        litellm_messages = []

        # å¤„ç†ç³»ç»Ÿæ¶ˆæ¯
        if anthropic_request.system:
            system_text = ""
            if isinstance(anthropic_request.system, str):
                system_text = anthropic_request.system
            elif isinstance(anthropic_request.system, list):
                text_parts = []
                for block in anthropic_request.system:
                    if hasattr(block, 'type') and block.type == "text":
                        text_parts.append(block.text)
                    elif isinstance(block, dict) and block.get("type") == "text":
                        text_parts.append(block.get("text", ""))
                system_text = "\n\n".join(text_parts)

            if system_text.strip():
                litellm_messages.append({"role": "system", "content": system_text.strip()})

        # å¤„ç†æ¶ˆæ¯
        for msg in anthropic_request.messages:
            if isinstance(msg.content, str):
                litellm_messages.append({"role": msg.role, "content": msg.content})
                continue

            # å¤„ç†å†…å®¹å—
            text_parts = []
            image_parts = []
            tool_calls = []
            pending_tool_messages = []

            for block in msg.content:
                if block.type == "text":
                    text_parts.append(block.text)
                elif block.type == "image":
                    if (isinstance(block.source, dict) and
                        block.source.get("type") == "base64" and
                        "media_type" in block.source and "data" in block.source):
                        image_parts.append({
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:{block.source['media_type']};base64,{block.source['data']}"
                            }
                        })
                elif block.type == "tool_use" and msg.role == "assistant":
                    tool_calls.append({
                        "id": block.id,
                        "type": "function",
                        "function": {
                            "name": block.name,
                            "arguments": json.dumps(block.input)
                        }
                    })
                elif block.type == "tool_result" and msg.role == "user":
                    # åˆ†å‰²ç”¨æˆ·æ¶ˆæ¯ä»¥å¤„ç†å·¥å…·ç»“æœ
                    if text_parts or image_parts:
                        content_parts = []
                        text_content = "".join(text_parts).strip()
                        if text_content:
                            content_parts.append({"type": "text", "text": text_content})
                        content_parts.extend(image_parts)

                        litellm_messages.append({
                            "role": "user",
                            "content": content_parts[0]["text"] if len(content_parts) == 1 and content_parts[0]["type"] == "text" else content_parts
                        })
                        text_parts.clear()
                        image_parts.clear()

                    # æ·»åŠ å·¥å…·ç»“æœä½œä¸ºå•ç‹¬çš„"tool"è§’è‰²æ¶ˆæ¯
                    parsed_content = parse_tool_result_content(block.content)
                    pending_tool_messages.append({
                        "role": "tool",
                        "tool_call_id": block.tool_use_id,
                        "content": parsed_content
                    })

            # æ ¹æ®è§’è‰²å®Œæˆæ¶ˆæ¯å¤„ç†
            if msg.role == "user":
                if text_parts or image_parts:
                    content_parts = []
                    text_content = "".join(text_parts).strip()
                    if text_content:
                        content_parts.append({"type": "text", "text": text_content})
                    content_parts.extend(image_parts)

                    litellm_messages.append({
                        "role": "user",
                        "content": content_parts[0]["text"] if len(content_parts) == 1 and content_parts[0]["type"] == "text" else content_parts
                    })
                litellm_messages.extend(pending_tool_messages)

            elif msg.role == "assistant":
                assistant_msg = {"role": "assistant"}

                content_parts = []
                text_content = "".join(text_parts).strip()
                if text_content:
                    content_parts.append({"type": "text", "text": text_content})
                content_parts.extend(image_parts)

                if content_parts:
                    assistant_msg["content"] = content_parts[0]["text"] if len(content_parts) == 1 and content_parts[0]["type"] == "text" else content_parts
                else:
                    assistant_msg["content"] = None

                if tool_calls:
                    assistant_msg["tool_calls"] = tool_calls

                if assistant_msg.get("content") or assistant_msg.get("tool_calls"):
                    litellm_messages.append(assistant_msg)

        # æ„å»ºLiteLLMè¯·æ±‚
        litellm_request = {
            "model": anthropic_request.model,
            "messages": litellm_messages,
            "max_tokens": min(anthropic_request.max_tokens, settings.MAX_TOKENS_LIMIT),
            "temperature": anthropic_request.temperature,
            "stream": anthropic_request.stream,
            "api_key": settings.GEMINI_API_KEY,
        }

        # æ·»åŠ å¯é€‰å‚æ•°
        if anthropic_request.stop_sequences:
            litellm_request["stop"] = anthropic_request.stop_sequences
        if anthropic_request.top_p is not None:
            litellm_request["top_p"] = anthropic_request.top_p
        if anthropic_request.top_k is not None:
            litellm_request["topK"] = anthropic_request.top_k

        # æ·»åŠ ä»£ç†é…ç½®
        if settings.effective_proxy_url:
            litellm_request["proxies"] = {"all://": settings.effective_proxy_url}

        # æ·»åŠ å·¥å…·
        if anthropic_request.tools:
            valid_tools = []
            for tool in anthropic_request.tools:
                if tool.name and tool.name.strip():
                    from .utils import StreamingHandler
                    cleaned_schema = StreamingHandler.clean_gemini_schema(tool.input_schema)
                    valid_tools.append({
                        "type": "function",
                        "function": {
                            "name": tool.name,
                            "description": tool.description or "",
                            "parameters": cleaned_schema
                        }
                    })
            if valid_tools:
                litellm_request["tools"] = valid_tools

        # æ·»åŠ å·¥å…·é€‰æ‹©é…ç½®
        if anthropic_request.tool_choice:
            choice_type = anthropic_request.tool_choice.get("type")
            if choice_type == "auto":
                litellm_request["tool_choice"] = "auto"
            elif choice_type == "any":
                litellm_request["tool_choice"] = "auto"
            elif choice_type == "tool" and "name" in anthropic_request.tool_choice:
                litellm_request["tool_choice"] = {
                    "type": "function",
                    "function": {"name": anthropic_request.tool_choice["name"]}
                }
            else:
                litellm_request["tool_choice"] = "auto"

        # æ·»åŠ æ€è€ƒé…ç½®
        if anthropic_request.thinking is not None:
            if anthropic_request.thinking.enabled:
                litellm_request["thinkingConfig"] = {"thinkingBudget": 24576}
            else:
                litellm_request["thinkingConfig"] = {"thinkingBudget": 0}

        # æ·»åŠ ç”¨æˆ·å…ƒæ•°æ®
        if (anthropic_request.metadata and
            "user_id" in anthropic_request.metadata and
            isinstance(anthropic_request.metadata["user_id"], str)):
            litellm_request["user"] = anthropic_request.metadata["user_id"]

        # æ·»åŠ è¶…æ—¶å’Œé‡è¯•é…ç½®
        litellm_request["timeout"] = settings.REQUEST_TIMEOUT
        litellm_request["num_retries"] = settings.MAX_RETRIES

        return litellm_request

    @staticmethod
    def convert_litellm_to_anthropic(litellm_response, original_request: MessagesRequest) -> MessagesResponse:
        """å°†LiteLLMå“åº”è½¬æ¢ä¸ºAnthropic APIæ ¼å¼"""
        try:
            response_id = f"msg_{uuid.uuid4()}"
            content_text = ""
            tool_calls = None
            finish_reason = "stop"
            prompt_tokens = 0
            completion_tokens = 0

            # å¤„ç†LiteLLM ModelResponseå¯¹è±¡æ ¼å¼
            if hasattr(litellm_response, 'choices') and hasattr(litellm_response, 'usage'):
                choices = litellm_response.choices
                message = choices[0].message if choices else None
                content_text = getattr(message, 'content', "") or ""
                tool_calls = getattr(message, 'tool_calls', None)
                finish_reason = choices[0].finish_reason if choices else "stop"
                response_id = getattr(litellm_response, 'id', response_id)

                if hasattr(litellm_response, 'usage'):
                    usage = litellm_response.usage
                    prompt_tokens = getattr(usage, "prompt_tokens", 0)
                    completion_tokens = getattr(usage, "completion_tokens", 0)

            # å¤„ç†å­—å…¸å“åº”æ ¼å¼
            elif isinstance(litellm_response, dict):
                choices = litellm_response.get("choices", [])
                message = choices[0].get("message", {}) if choices else {}
                content_text = message.get("content", "") or ""
                tool_calls = message.get("tool_calls")
                finish_reason = choices[0].get("finish_reason", "stop") if choices else "stop"
                usage = litellm_response.get("usage", {})
                prompt_tokens = usage.get("prompt_tokens", 0)
                completion_tokens = usage.get("completion_tokens", 0)
                response_id = litellm_response.get("id", response_id)

            # æ„å»ºå†…å®¹å—
            content_blocks = []

            # æ·»åŠ æ–‡æœ¬å†…å®¹
            if content_text:
                content_blocks.append(ContentBlockText(type="text", text=content_text))

            # å¤„ç†å·¥å…·è°ƒç”¨
            if tool_calls:
                if not isinstance(tool_calls, list):
                    tool_calls = [tool_calls]

                for tool_call in tool_calls:
                    try:
                        if isinstance(tool_call, dict):
                            tool_id = tool_call.get("id", f"tool_{uuid.uuid4()}")
                            function_data = tool_call.get("function", {})
                            name = function_data.get("name", "")
                            arguments_str = function_data.get("arguments", "{}")
                        elif hasattr(tool_call, "id") and hasattr(tool_call, "function"):
                            tool_id = tool_call.id
                            name = tool_call.function.name
                            arguments_str = tool_call.function.arguments
                        else:
                            continue

                        if not name:
                            continue

                        try:
                            arguments_dict = json.loads(arguments_str)
                        except json.JSONDecodeError:
                            arguments_dict = {"raw_arguments": arguments_str}

                        content_blocks.append(ContentBlockToolUse(
                            type="tool_use",
                            id=tool_id,
                            name=name,
                            input=arguments_dict
                        ))
                    except Exception as e:
                        logger.warning(f"å¤„ç†å·¥å…·è°ƒç”¨æ—¶å‡ºé”™: {e}")
                        continue

            # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªå†…å®¹å—
            if not content_blocks:
                content_blocks.append(ContentBlockText(type="text", text=""))

            # æ˜ å°„åœæ­¢åŸå› 
            if finish_reason == "length":
                stop_reason = "max_tokens"
            elif finish_reason == "tool_calls":
                stop_reason = "tool_use"
            elif finish_reason is None and tool_calls:
                stop_reason = "tool_use"
            else:
                stop_reason = "end_turn"

            return MessagesResponse(
                id=response_id,
                model=original_request.original_model or original_request.model,
                role="assistant",
                content=content_blocks,
                stop_reason=stop_reason,
                stop_sequence=None,
                usage=Usage(
                    input_tokens=prompt_tokens,
                    output_tokens=completion_tokens
                )
            )

        except Exception as e:
            logger.error(f"å“åº”è½¬æ¢é”™è¯¯: {e}")
            return MessagesResponse(
                id=f"msg_error_{uuid.uuid4()}",
                model=original_request.original_model or original_request.model,
                role="assistant",
                content=[ContentBlockText(type="text", text="å“åº”è½¬æ¢é”™è¯¯")],
                stop_reason="error",
                usage=Usage(input_tokens=0, output_tokens=0)
            )


# ===== åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç† =====
@asynccontextmanager
async def lifespan(app: FastAPI) -> AsyncGenerator[None, None]:
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    logger.info("ğŸš€ å¯åŠ¨å¢å¼ºç‰ˆClaude Code Geminiä»£ç†æœåŠ¡...")
    
    try:
        # éªŒè¯é…ç½®
        if not settings.GEMINI_API_KEY:
            raise ValueError("GEMINI_API_KEYç¯å¢ƒå˜é‡æœªè®¾ç½®")
        
        logger.info(f"âœ… é…ç½®éªŒè¯é€šè¿‡")
        logger.info(f"   æœåŠ¡åœ°å€: {settings.effective_host}:{settings.effective_port}")
        logger.info(f"   å¤§æ¨¡å‹: {settings.BIG_MODEL}")
        logger.info(f"   å°æ¨¡å‹: {settings.SMALL_MODEL}")
        logger.info(f"   å¯ç”¨Geminiæ¨¡å‹: {len(model_manager.gemini_models)}ä¸ª")
        logger.info(f"   ä»£ç†é…ç½®: {settings.effective_proxy_url or 'æœªå¯ç”¨'}")
        logger.info(f"   æµå¼é…ç½®: å¼ºåˆ¶ç¦ç”¨={settings.FORCE_DISABLE_STREAMING}, ç´§æ€¥ç¦ç”¨={settings.EMERGENCY_DISABLE_STREAMING}")
        
    except Exception as e:
        logger.error(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
        raise
    
    yield
    
    logger.info("ğŸ›‘ æ­£åœ¨å…³é—­æœåŠ¡...")


# ===== åˆ›å»ºFastAPIåº”ç”¨ =====
app = FastAPI(
    title="å¢å¼ºç‰ˆClaude Code Geminiä»£ç†",
    description="åŸºäºserver.pyå®ç°çš„é«˜æ€§èƒ½Claude Codeä»£ç†æœåŠ¡ï¼Œæ”¯æŒå®Œæ•´çš„APIç«¯ç‚¹å’Œæµå¼å¤„ç†",
    version="2.5.0",
    lifespan=lifespan
)

# æ·»åŠ ä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)


# ===== è¯·æ±‚æ—¥å¿—ä¸­é—´ä»¶ =====
@app.middleware("http")
async def log_requests(request: Request, call_next):
    """è¯·æ±‚æ—¥å¿—ä¸­é—´ä»¶"""
    start_time = time.time()
    method = request.method
    path = request.url.path
    
    logger.debug(f"ğŸ“¥ è¯·æ±‚: {method} {path}")
    
    response = await call_next(request)
    
    duration = time.time() - start_time
    logger.debug(f"ğŸ“¤ å“åº”: {method} {path} - {response.status_code} ({duration:.2f}s)")
    
    return response


# ===== APIç«¯ç‚¹ =====
@app.get("/")
async def root():
    """æ ¹è·¯å¾„ï¼Œè¿”å›æœåŠ¡ä¿¡æ¯"""
    return {
        "service": "å¢å¼ºç‰ˆClaude Code Geminiä»£ç†",
        "version": "2.5.0",
        "status": "running",
        "description": "åŸºäºserver.pyå®ç°çš„é«˜æ€§èƒ½ä»£ç†æœåŠ¡",
        "endpoints": {
            "messages": "/v1/messages",
            "count_tokens": "/v1/messages/count_tokens",
            "health": "/health",
            "test_connection": "/test-connection",
            "models": "/models"
        },
        "configuration": {
            "big_model": settings.BIG_MODEL,
            "small_model": settings.SMALL_MODEL,
            "available_models": model_manager.gemini_models,
            "proxy_enabled": bool(settings.effective_proxy_url),
            "streaming_config": {
                "force_disabled": settings.FORCE_DISABLE_STREAMING,
                "emergency_disabled": settings.EMERGENCY_DISABLE_STREAMING,
                "max_retries": settings.MAX_STREAMING_RETRIES
            }
        },
        "docs": "/docs"
    }


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    try:
        health_status = await HealthChecker.check_system_health()
        
        if health_status["healthy"]:
            return health_status
        else:
            return JSONResponse(
                status_code=503,
                content=health_status
            )
            
    except Exception as e:
        logger.error(f"å¥åº·æ£€æŸ¥é”™è¯¯: {e}")
        return JSONResponse(
            status_code=503,
            content={
                "healthy": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
        )


@app.get("/models")
async def get_available_models():
    """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
    return {
        "models": model_manager.gemini_models,
        "big_model": settings.BIG_MODEL,
        "small_model": settings.SMALL_MODEL,
        "timestamp": datetime.now().isoformat()
    }


@app.get("/test-connection")
async def test_connection():
    """æµ‹è¯•Gemini APIè¿æ¥"""
    try:
        result = await HealthChecker.test_gemini_api()
        return result
    except Exception as e:
        logger.error(f"è¿æ¥æµ‹è¯•é”™è¯¯: {e}")
        error_info = error_handler.get_error_solution(
            error_handler.classify_error(str(e)), str(e)
        )
        return JSONResponse(
            status_code=503,
            content={
                "status": "failed",
                "error_type": error_info["error_type"],
                "message": error_info["message"],
                "solution": error_info["solution"],
                "timestamp": datetime.now().isoformat()
            }
        )


@app.post("/v1/messages")
async def create_message(request: MessagesRequest, raw_request: Request):
    """
    ä¸»æ¶ˆæ¯å¤„ç†ç«¯ç‚¹
    å…¼å®¹Anthropic Messages APIæ ¼å¼
    """
    request_id = create_request_id()
    
    try:
        logger.info(f"ğŸ“Š å¤„ç†è¯·æ±‚ - ID: {request_id}, æ¨¡å‹: {request.original_model}, æµå¼: {request.stream}")

        # æ£€æŸ¥æµå¼é…ç½®
        if request.stream and settings.EMERGENCY_DISABLE_STREAMING:
            logger.warning("âš ï¸ æµå¼å“åº”é€šè¿‡EMERGENCY_DISABLE_STREAMINGè¢«ç¦ç”¨")
            request.stream = False

        if request.stream and settings.FORCE_DISABLE_STREAMING:
            logger.info("â„¹ï¸ æµå¼å“åº”é€šè¿‡FORCE_DISABLE_STREAMINGè¢«ç¦ç”¨")
            request.stream = False
        # è½¬æ¢è¯·æ±‚
        litellm_request = RequestConverter.convert_anthropic_to_litellm(request)
        
        # æ—¥å¿—è®°å½•
        num_tools = len(request.tools) if request.tools else 0
        LoggingManager.log_api_request(
            request_id, "POST", raw_request.url.path,
            request.original_model or request.model,
            len(litellm_request['messages']),
            num_tools
        )

        # logger.info(f"Converted request: {litellm_request.}")

        # å¤„ç†æµå¼å“åº”
        if request.stream:
            return await handle_streaming_response(litellm_request, request, request_id)
        else:
            # å¤„ç†éæµå¼å“åº”
            return await handle_non_streaming_response(litellm_request, request, request_id)

    except litellm.exceptions.APIError as e:
        logger.error(f"ğŸ”´ LiteLLM APIé”™è¯¯ - ID: {request_id}: {e}")
        error_msg = error_handler.get_error_solution(
            error_handler.classify_error(str(e)), str(e)
        )
        raise HTTPException(status_code=getattr(e, 'status_code', 500), detail=error_msg["message"])
    
    except ConnectionError as e:
        logger.error(f"ğŸ”Œ è¿æ¥é”™è¯¯ - ID: {request_id}: {e}")
        raise HTTPException(status_code=503, detail="è¿æ¥é”™è¯¯ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
    
    except TimeoutError as e:
        logger.error(f"â° è¶…æ—¶é”™è¯¯ - ID: {request_id}: {e}")
        raise HTTPException(status_code=504, detail="è¯·æ±‚è¶…æ—¶ï¼Œè¯·é‡è¯•")
    
    except Exception as e:
        logger.error(f"âŒ å¤„ç†è¯·æ±‚æ—¶å‡ºé”™ - ID: {request_id}: {e}")
        error_msg = error_handler.get_error_solution(
            error_handler.classify_error(str(e)), str(e)
        )
        raise HTTPException(status_code=500, detail=error_msg["message"])


async def handle_streaming_response(litellm_request: Dict[str, Any], 
                                  original_request: MessagesRequest,
                                  request_id: str):
    """å¤„ç†æµå¼å“åº”"""
    streaming_retry_count = 0
    max_retries = settings.MAX_STREAMING_RETRIES

    while streaming_retry_count <= max_retries:
        try:
            if streaming_retry_count > 0:
                delay = min(0.5 * (2 ** streaming_retry_count), 2.0)
                logger.debug(f"â³ é‡è¯•ç­‰å¾…: {delay}s (å°è¯• {streaming_retry_count}/{max_retries})")
                await asyncio.sleep(delay)

            response_generator = await litellm.acompletion(**litellm_request)

            return StreamingResponse(
                streaming_handler.handle_streaming_with_recovery(response_generator, request_id, original_request.original_model or original_request.model),
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "X-Accel-Buffering": "no",
                    "Access-Control-Allow-Origin": "*",
                    "Access-Control-Allow-Headers": "*",
                    "X-Request-ID": request_id
                }
            )

        except Exception as streaming_error:
            streaming_retry_count += 1
            logger.warning(f"ğŸ”„ æµå¼å“åº”é‡è¯• {streaming_retry_count}/{max_retries}: {streaming_error}")

            if streaming_retry_count > max_retries:
                logger.error("âŒ æµå¼å“åº”å¤±è´¥ï¼Œå›é€€åˆ°éæµå¼æ¨¡å¼")
                break

    # å›é€€åˆ°éæµå¼å“åº”
    logger.info("ğŸ“¥ å›é€€åˆ°éæµå¼æ¨¡å¼")
    litellm_request["stream"] = False
    return await handle_non_streaming_response(litellm_request, original_request, request_id)


async def handle_non_streaming_response(litellm_request: Dict[str, Any],
                                     original_request: MessagesRequest,
                                     request_id: str):
    """å¤„ç†éæµå¼å“åº”"""
    start_time = time.time()
    
    litellm_response = await retry_manager.execute_with_retry(
        litellm.acompletion, **litellm_request
    )
    
    duration = time.time() - start_time
    anthropic_response = RequestConverter.convert_litellm_to_anthropic(litellm_response, original_request)
    
    # æ—¥å¿—è®°å½•
    LoggingManager.log_api_response(
        request_id, duration,
        {
            "input_tokens": anthropic_response.usage.input_tokens,
            "output_tokens": anthropic_response.usage.output_tokens
        },
        "success"
    )
    
    return anthropic_response


@app.post("/v1/messages/count_tokens")
async def count_tokens(request: TokenCountRequest, raw_request: Request):
    """Tokenè®¡æ•°ç«¯ç‚¹"""
    request_id = create_request_id()
    
    try:
        logger.info(f"ğŸ“Š Tokenè®¡æ•° - ID: {request_id}, æ¨¡å‹: {request.original_model}")

        # åˆ›å»ºä¸´æ—¶è¯·æ±‚è¿›è¡Œè½¬æ¢
        temp_request = MessagesRequest(
            model=request.model,
            max_tokens=1,
            messages=request.messages,
            system=request.system,
            tools=request.tools,
        )

        litellm_data = RequestConverter.convert_anthropic_to_litellm(temp_request)

        # æ—¥å¿—è®°å½•
        num_tools = len(request.tools) if request.tools else 0
        LoggingManager.log_api_request(
            request_id, "POST", raw_request.url.path,
            request.original_model or request.model,
            len(litellm_data['messages']),
            num_tools
        )

        # è®¡ç®—tokenæ•°
        token_count = litellm.token_counter(
            model=litellm_data["model"],
            messages=litellm_data["messages"],
        )

        response = TokenCountResponse(input_tokens=token_count)
        
        LoggingManager.log_api_response(
            request_id, 0.1, {"input_tokens": token_count}, "success"
        )
        
        return response

    except Exception as e:
        logger.error(f"âŒ Tokenè®¡æ•°é”™è¯¯ - ID: {request_id}: {e}")
        error_msg = error_handler.get_error_solution(
            error_handler.classify_error(str(e)), str(e)
        )
        raise HTTPException(status_code=500, detail=f"Tokenè®¡æ•°é”™è¯¯: {error_msg['message']}")


# ===== å¯åŠ¨å‡½æ•° =====
def validate_startup():
    """å¯åŠ¨æ—¶éªŒè¯é…ç½®"""
    print("ğŸ” éªŒè¯å¯åŠ¨é…ç½®...")
    
    # æ£€æŸ¥APIå¯†é’¥
    if not settings.GEMINI_API_KEY:
        print("âŒ è‡´å‘½é”™è¯¯: GEMINI_API_KEYæœªè®¾ç½®")
        return False
    
    if not settings.validate_api_key():
        print("âš ï¸ è­¦å‘Š: APIå¯†é’¥æ ¼å¼éªŒè¯å¤±è´¥")
    
    # æ£€æŸ¥ç½‘ç»œè¿æ¥
    try:
        import socket
        socket.create_connection(("8.8.8.8", 53), timeout=10)
        print("âœ… ç½‘ç»œè¿æ¥æ­£å¸¸")
    except OSError:
        print("âš ï¸ è­¦å‘Š: ç½‘ç»œè¿æ¥æ£€æŸ¥å¤±è´¥")
    
    return True


if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] == "--help":
        print("å¢å¼ºç‰ˆClaude Code Geminiä»£ç† v2.5.0")
        print("")
        print("ä½¿ç”¨æ–¹å¼: uvicorn src.main:app --reload --host 0.0.0.0 --port 8080")
        print("")
        print("å¿…éœ€ç¯å¢ƒå˜é‡:")
        print("  GEMINI_API_KEY - Google Gemini APIå¯†é’¥")
        print("")
        print("å¯é€‰ç¯å¢ƒå˜é‡:")
        print(f"  BIG_MODEL - å¤§æ¨¡å‹åç§° (é»˜è®¤: {settings.BIG_MODEL})")
        print(f"  SMALL_MODEL - å°æ¨¡å‹åç§° (é»˜è®¤: {settings.SMALL_MODEL})")
        print(f"  HOST - æœåŠ¡å™¨åœ°å€ (é»˜è®¤: {settings.HOST})")
        print(f"  PORT - æœåŠ¡å™¨ç«¯å£ (é»˜è®¤: {settings.PORT})")
        print(f"  LOG_LEVEL - æ—¥å¿—çº§åˆ« (é»˜è®¤: {settings.LOG_LEVEL})")
        print(f"  PROXY_ENABLED - æ˜¯å¦å¯ç”¨ä»£ç† (é»˜è®¤: {settings.PROXY_ENABLED})")
        print(f"  PROXY_URL - ä»£ç†åœ°å€ (é»˜è®¤: {settings.PROXY_URL})")
        sys.exit(0)

    # éªŒè¯å¯åŠ¨é…ç½®
    if not validate_startup():
        print("âŒ å¯åŠ¨éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
        sys.exit(1)

    # é…ç½®æ‘˜è¦
    print("ğŸš€ å¢å¼ºç‰ˆClaude Code Geminiä»£ç† v2.5.0")
    print("âœ… é…ç½®éªŒè¯é€šè¿‡")
    print(f"   å¤§æ¨¡å‹: {settings.BIG_MODEL}")
    print(f"   å°æ¨¡å‹: {settings.SMALL_MODEL}")
    print(f"   å¯ç”¨æ¨¡å‹: {len(model_manager.gemini_models)}ä¸ª")
    print(f"   æœåŠ¡åœ°å€: {settings.effective_host}:{settings.effective_port}")
    print(f"   ä»£ç†é…ç½®: {settings.effective_proxy_url or 'æœªå¯ç”¨'}")
    print(f"   æµå¼é…ç½®: é‡è¯•æ¬¡æ•°={settings.MAX_STREAMING_RETRIES}")
    print("")

    # å¯åŠ¨æœåŠ¡å™¨
    uvicorn.run(
        "src.main:app",
        host=settings.effective_host,
        port=settings.effective_port,
        log_level=settings.LOG_LEVEL.lower(),
        reload=True
    )